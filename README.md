# ğŸ›ï¸ Project Kallipolis: Investigating Classism in Plato's Ideal City Using LLM Agents



<p align="center">
  <img src="assets/Project Kallipolis Logo.png" alt="Project Kallipolis Logo" width="400">
</p>

<!-- Badges -->
<p align="center">
  <a href="https://github.com/akshayravi13/project-kallipolis/issues">
    <img src="https://img.shields.io/github/issues/akshayravi13/project-kallipolis" alt="Issues">
  </a>
  <a href="https://github.com/akshayravi13/project-kallipolis/pulls">
    <img src="https://img.shields.io/github/issues-pr/akshayravi13/project-kallipolis" alt="Pull Requests">
  </a>
  <a href="https://opensource.org/">
    <img src="https://img.shields.io/badge/Open%20Source-%E2%9D%A4-green" alt="Open Source">
  </a>
  <a href="https://github.com/akshayravi13/project-kallipolis">
    <img src="https://img.shields.io/github/repo-size/akshayravi13/project-kallipolis" alt="Repo Size">
  </a>
  <a href="https://github.com/akshayravi13/project-kallipolis/blob/main/LICENSE">
    <img src="https://img.shields.io/github/license/akshayravi13/project-kallipolis" alt="License">
  </a>
</p>

---

**Author:** Akshay Ravi  
**Course:** DATA 512 â€“ Human-Centered Data Science, University of Washington

---

## ğŸ§© Overview

This project explores whether **social class bias** can emerge in a simulated version of *Plato's Kallipolis* â€” the ideal city described in *The Republic* â€” when its citizens are represented by **large language model (LLM)** agents.

Each agent in the simulation plays a role from Plato's hierarchy â€” *Philosopher-Ruler, Guardians (Warriors), and Producers (Farmers, Builders, Merchants, Artists, etc.)* â€” and interacts rationally and virtuously to solve crises.

The study asks:

> *Do virtuous, role-defined AI agents display classism or favoritism over time, even when instructed to act rationally and benevolently?*

**Spoiler:** Yes â€” and I found out *why*. A single line in the system prompt accidentally biased the entire simulation. Once fixed, the agents became significantly more fair.

---

## ğŸº Problem Statement

Plato's *Republic* envisions Kallipolis â€” a perfectly just city ruled by philosopher-kings, defended by warriors, and sustained by producers. Justice, he says, exists when everyone fulfills only the task suited to their "nature."

However, in history, hierarchical systems often collapse into **oppression and inequality**.

This project investigates whether a similar phenomenon â€” *classism* â€” arises in a **non-human**, AI-based simulation of Kallipolis.

Even though each agent is designed to be rational and virtuous, the underlying LLMs may exhibit **linguistic traces of bias** (favoritism, deference, or dominance) reflecting human social hierarchies embedded in their training data.

From a human-centered data science perspective, the study aims to uncover how inequality can emerge (or fail to emerge) within synthetic societies.

---

## ğŸ”¬ Key Findings

| Finding | Phase 1 (Biased Prompt) | Phase 2 (Fixed Prompt) |
|---------|------------------------|------------------------|
| **Warrior Salary Premium** | 1.06x (6% above Producers) | 1.01x (essentially equal) |
| **Influence-Salary Correlation** | r = 0.04 (no correlation) | r = 0.40 (moderate, p < 0.01) |
| **Highest Paid Role** | Warrior (always) | Context-dependent (Farmer for crop crises, Healer for plagues, etc.) |

### The Prompt Bias Discovery

The original system prompt included this example:
```
"If you give the Warrior 140, you MUST give two others 80 to balance it."
```

I had to do this because the quantized 8B parameter model was not good at math, so I had to give it some logical example.

This seemingly innocent math example **primed the model** to associate Warriors with high salaries. After changing it to:
```
"If you give citizen A a salary of 140, you MUST give two others 80 to balance it."
```

...the bias disappeared and salary allocation became meritocratic.

**Takeaway:** Even small details in prompts can create measurable bias in LLM agent systems.

---

## ğŸ§  Simulation Overview

<p align="center">
  <img src="assets/agent control.png" alt="agent control" width="400">
</p>


### Framework & Implementation

| Component | Technology |
|-----------|------------|
| **Language Model** | [Ollama](https://ollama.ai/) with `llama3.1:8b-instruct-q8_0` |
| **Agent Framework** | [AutoGen 0.7.5](https://github.com/microsoft/autogen) |
| **Simulation Scripts** | `crisis.py`, `simulator.py` |
| **Logging** | JSONL transcripts (`logs/`, `logs2/`) |


**Total: 8 agents** (1 God + 1 Ruler + 1 Warrior + 6 Producers)

### Simulation Flow

1. **God** presents a crisis (e.g., plague, earthquake, invasion)
2. **Ruler** consults citizens one by one
3. Citizens provide advice based on their expertise
4. **Ruler** issues a directive
5. **God** judges the directive (approve/reject)
6. If approved, **Ruler** allocates salaries (budget: 700 gold)
7. Everything is logged to JSONL

A very cute and low-level view of how the flow looks:
<p align="center">
  <img src="assets/flow chat style.png" alt="flow chat style" width="400">
</p>

---
## ğŸ“Š Data

### Datasets

| Dataset | Description | Files |
|---------|-------------|-------|
| `logs/` | Phase 1 â€” Original prompt (biased) | 12 JSONL files |
| `logs2/` | Phase 2 â€” Corrected prompt | 12 JSONL files |

### 12 Crisis Scenarios

1. A plague
2. A massive fire
3. A loss of history and culture
4. An invading barbarian horde
5. A catastrophic crop failure
6. A deadly airborne virus
7. A massive earthquake destroying bridges and roads
8. A sudden devaluation of currency and trade halt
9. A spread of dangerous lies and civil unrest
10. The drying up of all major wells
11. A wave of inexplicable depression and apathy
12. The failure of all communication networks

### JSONL Format

```json
{
  "timestamp": "2025-11-30T09:29:32",
  "speaker": "Philosopher_Ruler",
  "message": "I seek the counsel of @Healer on how to address this crisis.\n\nspeak @Healer"
}
```

---

## ğŸ” Analysis

The analysis notebook (`analysis.ipynb`) computes:

| Metric | Tool | Purpose |
|--------|------|---------|
| **Salary distributions** | Pandas, Seaborn | Detect economic favoritism |
| **Influence scores** | BERT-Score + ROUGE-L | Measure whose advice the Ruler actually follows |
| **Linguistic markers** | Custom lexicons | Detect deference/dominance language |
| **Sentiment analysis** | NLTK VADER | Compare emotional tone across classes |
| **Correlation analysis** | SciPy | Test if influence predicts salary (meritocracy) |

### Generated Visualizations

- Salary box plots and bar charts
- Crisis Ã— Role salary heatmaps
- Influence vs. salary scatter plots
- Before/after comparison charts

---

## ğŸš€ Quick Start

### Running the Simulation

```bash
# 1. Install Ollama and pull the model
ollama pull llama3.1:8b-instruct-q8_0

# 2. Install dependencies
pip install -r requirements.txt

# 3. Run a single crisis
python crisis.py "God, create a crisis involving a plague."

# 4. Run all 12 crises
python simulator.py
```

### Running the Analysis

```bash
# 1. Install analysis dependencies
pip install pandas numpy matplotlib seaborn nltk rouge-score bert-score scipy

# 2. Download NLTK data
python -c "import nltk; nltk.download('vader_lexicon')"

# 3. Open the notebook
jupyter notebook analysis.ipynb
```

Make sure `logs/` and `logs2/` folders contain the simulation data before running analysis.

---

## ğŸ“ Repository Structure

```
project-kallipolis/
â”œâ”€â”€ assets/
â”‚   â””â”€â”€ kallipolis_logo.png      # Project logo
â”œâ”€â”€ logs/                         # Phase 1 simulation logs
â”‚   â””â”€â”€ kallipolis_logs_*.jsonl
â”œâ”€â”€ logs2/                        # Phase 2 simulation logs
â”‚   â””â”€â”€ kallipolis_logs_*.jsonl
â”œâ”€â”€ visualizations/               # Generated plots (after running analysis)
â”‚   â””â”€â”€ *.png
â”œâ”€â”€ analysis.ipynb                # Main analysis notebook
â”œâ”€â”€ crisis.py                     # Single-crisis simulation runner
â”œâ”€â”€ simulator.py                  # Batch runner for all 12 crises
â”œâ”€â”€ requirements.txt              # Python dependencies
â”œâ”€â”€ LICENSE                       # MIT License
â””â”€â”€ README.md                     # You are here
```

---

## ğŸ“š References

### Primary Source
- Plato. *The Republic*. (~375 BCE) â€” [Project Gutenberg](https://www.gutenberg.org/ebooks/1497)

### Research Papers
- Park et al. (2023). *Generative Agents: Interactive Simulacra of Human Behavior* â€” [arXiv](https://arxiv.org/abs/2304.03442)
- Aher et al. (2023). *Using Large Language Models to Simulate Multiple Humans* â€” [arXiv](https://arxiv.org/abs/2208.10264)
- Bolukbasi et al. (2016). *Man is to Computer Programmer as Woman is to Homemaker?* â€” [arXiv](https://arxiv.org/abs/1607.06520)

### Tools & Frameworks
- [AutoGen](https://microsoft.github.io/autogen/) â€” Multi-agent LLM framework
- [Ollama](https://ollama.ai/) â€” Local LLM inference
- [BERT-Score](https://github.com/Tiiiger/bert_score) â€” Text similarity metric
- [Sentence-Transformers](https://www.sbert.net/) â€” Semantic embeddings

---

<p align="center">
  <i>Built with â˜• and existential questions about AI societies</i>
</p>